{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Asynchronous federated learning on MNIST\n",
    "\n",
    "This notebook will go through the steps to run a federated learning via websocket workers in an asynchronous way using [TrainConfig](https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/advanced/Federated%20Learning%20with%20TrainConfig/Introduction%20to%20TrainConfig.ipynb). We will use federated averaging to join the remotely trained models.\n",
    "\n",
    "Authors:\n",
    "- Silvia - GitHub [@midokura-silvia](https://github.com/midokura-silvia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning setup\n",
    "\n",
    "For a Federated Learning setup with TrainConfig we need different participants:\n",
    "\n",
    "* _Workers_: own datasets.\n",
    "\n",
    "* _Coordinator_: an entity that knows the workers and the dataset name that lives in each worker. \n",
    "\n",
    "* _Evaluator_: holds the testing data and tracks model performance \n",
    "\n",
    "Each worker is represented by two parts, a proxy local to the scheduler (websocket client worker) and the remote instance that holds the data and performs the computations. The remote part is called a websocket server worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: Start the websocket workers\n",
    "So first, we need to create the remote workers. For this, you need to run in a terminal (not possible from the notebook):\n",
    "\n",
    "```bash\n",
    "python start_websocket_servers.py\n",
    "```\n",
    "\n",
    "#### What's going on?\n",
    "\n",
    "The script will instantiate three workers, Alice, Bob and Charlie and prepare their local data. \n",
    "Each worker is set up to have a subset of the MNIST training dataset. \n",
    "Alice holds all images corresponding to the digits 0-3, \n",
    "Bob holds all images corresponding to the digits 4-6 and \n",
    "Charlie holds all images corresponding to the digits 7-9. \n",
    "\n",
    "| Worker      | Digits in local dataset | Number of samples |\n",
    "| ----------- | ----------------------- | ----------------- |\n",
    "| Alice       | 0-3                     | 24754             |\n",
    "| Bob         | 4-6                     | 17181             |\n",
    "| Charlie     | 7-9                     | 18065             |\n",
    "\n",
    "\n",
    "The evaluator will be called Testing and holds the entire MNIST testing dataset.\n",
    "\n",
    "| Evaluator   | Digits in local dataset | Number of samples |\n",
    "| ----------- | ----------------------- | ----------------- |\n",
    "| Testing     | 0-9                     | 10000             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following to see the code of the function that starts a worker\n",
    "# import run_websocket_server\n",
    "\n",
    "# print(inspect.getsource(run_websocket_server.start_websocket_server_worker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing let's first need to import dependencies, setup needed arguments and configure logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import sys\n",
    "import asyncio\n",
    "\n",
    "import syft as sy\n",
    "from syft.workers.websocket_client import WebsocketClientWorker\n",
    "from syft.frameworks.torch.fl import utils\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "import run_websocket_client as rwc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook torch\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "Namespace(batch_size=512, cuda=False, federate_after_n_batches=10, lr=0.8, save_model=False, seed=1, test_batch_size=128, training_rounds=1000, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "args = rwc.define_and_get_arguments(args=[])\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(torch.cuda.is_available())\n",
    "torch.manual_seed(args.seed)\n",
    "print(use_cuda)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"run_websocket_client\")\n",
    "\n",
    "if not len(logger.handlers):\n",
    "    FORMAT = \"%(asctime)s - %(message)s\"\n",
    "    DATE_FMT = \"%H:%M:%S\"\n",
    "    formatter = logging.Formatter(FORMAT, DATE_FMT)\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.propagate = False\n",
    "LOG_LEVEL = logging.DEBUG\n",
    "logger.setLevel(LOG_LEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's instantiate the websocket client workers, our local proxies to the remote workers.\n",
    "Note that **this step will fail, if the websocket server workers are not running**.\n",
    "\n",
    "The workers Alice, Bob and Charlie will perform the training, wheras the testing worker hosts the test data and performs the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kwargs_websocket = {\"host\": \"106.12.19.48\", \"hook\": hook, \"verbose\": args.verbose}\n",
    "#alice = WebsocketClientWorker(id=\"alice\", port=28032, **kwargs_websocket)\n",
    "\n",
    "kwargs_websocket = {\"host\": \"106.12.19.48\", \"hook\": hook, \"verbose\": args.verbose}\n",
    "testing = WebsocketClientWorker(id=\"testing\", port=28007, **kwargs_websocket)\n",
    "\n",
    "kwargs_websocket = {\"host\": \"106.12.19.48\", \"hook\": hook, \"verbose\": args.verbose}\n",
    "bob = WebsocketClientWorker(id=\"bob\", port=28048, **kwargs_websocket)\n",
    "#charlie = WebsocketClientWorker(id=\"charlie\", port=8779, **kwargs_websocket)\n",
    "\n",
    "worker_instances = [ bob]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Let's instantiate the machine learning model. It is a small neural network with 2 convolutional and two fully connected layers. \n",
    "It uses ReLU activations and max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(inspect.getsource(rwc.Net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=470, out_features=5, bias=True)\n",
      "  (fc2): Linear(in_features=5, out_features=5, bias=True)\n",
      "  (fc4): Linear(in_features=5, out_features=5, bias=True)\n",
      "  (Tanh): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = rwc.Net().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the model serializable\n",
    "\n",
    "In order to send the model to the workers we need the model to be serializable, for this we use [`jit`](https://pytorch.org/docs/stable/jit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(model, torch.zeros([1, 1, 470], dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start the training\n",
    "\n",
    "Now we are ready to start the federated training. We will perform training over a given number of batches separately on each worker and then calculate the federated average of the resulting model.\n",
    "\n",
    "Every 10th training round we will evaluate the performance of the models returned by the workers and of the model obtained by federated averaging. \n",
    "\n",
    "The performance will be given both as the accuracy (ratio of correct predictions) and as the histograms of predicted digits. This is of interest, as each worker only owns a subset of the digits. Therefore, in the beginning each worker will only predict their numbers and only know about the other numbers via the federated averaging process.\n",
    "\n",
    "The training is done in an asynchronous manner. This means that the scheduler just tell the workers to train and does not block to wait for the result of the training before talking to the next worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the training are given in the arguments. \n",
    "Each worker will train on a given number of batches, given by the value of federate_after_n_batches.\n",
    "The training batch size and learning rate are also configured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federate_after_n_batches: 10\n",
      "Batch size: 512\n",
      "Initial learning rate: 0.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Federate_after_n_batches: \" + str(args.federate_after_n_batches))\n",
    "print(\"Batch size: \" + str(args.batch_size))\n",
    "print(\"Initial learning rate: \" + str(args.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:28:19 - Training round 1/1000\n",
      "08:28:20 - Evaluating models\n",
      "08:28:20 - Model update bob: Average loss: 0.0121, Accuracy: 587/1598 (36.73%)\n",
      "08:28:20 - Model update bob: Average loss: 0.0121\n",
      "08:28:21 - Federated model: Average loss: 0.0121, Accuracy: 587/1598 (36.73%)\n",
      "08:28:21 - Federated model: Average loss: 0.0121\n",
      "08:28:21 - Training round 2/1000\n",
      "08:28:22 - Training round 3/1000\n",
      "08:28:23 - Training round 4/1000\n",
      "08:28:24 - Training round 5/1000\n",
      "08:28:25 - Training round 6/1000\n",
      "08:28:26 - Training round 7/1000\n",
      "08:28:27 - Training round 8/1000\n",
      "08:28:28 - Training round 9/1000\n",
      "08:28:29 - Training round 10/1000\n",
      "08:28:30 - Training round 11/1000\n",
      "08:28:31 - Evaluating models\n",
      "08:28:32 - Model update bob: Average loss: 0.0098, Accuracy: 841/1598 (52.63%)\n",
      "08:28:32 - Model update bob: Average loss: 0.0098\n",
      "08:28:32 - Federated model: Average loss: 0.0098, Accuracy: 841/1598 (52.63%)\n",
      "08:28:32 - Federated model: Average loss: 0.0098\n",
      "08:28:32 - Training round 12/1000\n",
      "08:28:33 - Training round 13/1000\n",
      "08:28:34 - Training round 14/1000\n",
      "08:28:35 - Training round 15/1000\n",
      "08:28:36 - Training round 16/1000\n",
      "08:28:37 - Training round 17/1000\n",
      "08:28:38 - Training round 18/1000\n",
      "08:28:39 - Training round 19/1000\n",
      "08:28:40 - Training round 20/1000\n",
      "08:28:41 - Training round 21/1000\n",
      "08:28:42 - Evaluating models\n",
      "08:28:42 - Model update bob: Average loss: 0.0084, Accuracy: 929/1598 (58.14%)\n",
      "08:28:42 - Model update bob: Average loss: 0.0084\n",
      "08:28:43 - Federated model: Average loss: 0.0084, Accuracy: 929/1598 (58.14%)\n",
      "08:28:43 - Federated model: Average loss: 0.0084\n",
      "08:28:43 - Training round 22/1000\n",
      "08:28:44 - Training round 23/1000\n",
      "08:28:45 - Training round 24/1000\n",
      "08:28:46 - Training round 25/1000\n",
      "08:28:47 - Training round 26/1000\n",
      "08:28:48 - Training round 27/1000\n",
      "08:28:49 - Training round 28/1000\n",
      "08:28:50 - Training round 29/1000\n",
      "08:28:51 - Training round 30/1000\n",
      "08:28:52 - Training round 31/1000\n",
      "08:28:53 - Evaluating models\n",
      "08:28:54 - Model update bob: Average loss: 0.0086, Accuracy: 959/1598 (60.01%)\n",
      "08:28:54 - Model update bob: Average loss: 0.0086\n",
      "08:28:54 - Federated model: Average loss: 0.0086, Accuracy: 959/1598 (60.01%)\n",
      "08:28:54 - Federated model: Average loss: 0.0086\n",
      "08:28:54 - Training round 32/1000\n",
      "08:28:55 - Training round 33/1000\n",
      "08:28:56 - Training round 34/1000\n",
      "08:28:57 - Training round 35/1000\n",
      "08:28:58 - Training round 36/1000\n",
      "08:28:59 - Training round 37/1000\n",
      "08:29:00 - Training round 38/1000\n",
      "08:29:01 - Training round 39/1000\n",
      "08:29:02 - Training round 40/1000\n",
      "08:29:03 - Training round 41/1000\n",
      "08:29:04 - Evaluating models\n",
      "08:29:05 - Model update bob: Average loss: 0.0077, Accuracy: 995/1598 (62.27%)\n",
      "08:29:05 - Model update bob: Average loss: 0.0077\n",
      "08:29:05 - Federated model: Average loss: 0.0077, Accuracy: 995/1598 (62.27%)\n",
      "08:29:05 - Federated model: Average loss: 0.0077\n",
      "08:29:05 - Training round 42/1000\n",
      "08:29:06 - Training round 43/1000\n",
      "08:29:07 - Training round 44/1000\n",
      "08:29:08 - Training round 45/1000\n",
      "08:29:09 - Training round 46/1000\n",
      "08:29:10 - Training round 47/1000\n",
      "08:29:11 - Training round 48/1000\n",
      "08:29:12 - Training round 49/1000\n",
      "08:29:13 - Training round 50/1000\n",
      "08:29:14 - Training round 51/1000\n",
      "08:29:15 - Evaluating models\n",
      "08:29:16 - Model update bob: Average loss: 0.0071, Accuracy: 1035/1598 (64.77%)\n",
      "08:29:16 - Model update bob: Average loss: 0.0071\n",
      "08:29:16 - Federated model: Average loss: 0.0071, Accuracy: 1035/1598 (64.77%)\n",
      "08:29:16 - Federated model: Average loss: 0.0071\n",
      "08:29:16 - Training round 52/1000\n",
      "08:29:17 - Training round 53/1000\n",
      "08:29:18 - Training round 54/1000\n",
      "08:29:19 - Training round 55/1000\n",
      "08:29:20 - Training round 56/1000\n",
      "08:29:21 - Training round 57/1000\n",
      "08:29:22 - Training round 58/1000\n",
      "08:29:23 - Training round 59/1000\n",
      "08:29:24 - Training round 60/1000\n",
      "08:29:25 - Training round 61/1000\n",
      "08:29:26 - Evaluating models\n",
      "08:29:27 - Model update bob: Average loss: 0.0072, Accuracy: 1095/1598 (68.52%)\n",
      "08:29:27 - Model update bob: Average loss: 0.0072\n",
      "08:29:27 - Federated model: Average loss: 0.0072, Accuracy: 1095/1598 (68.52%)\n",
      "08:29:27 - Federated model: Average loss: 0.0072\n",
      "08:29:27 - Training round 62/1000\n",
      "08:29:28 - Training round 63/1000\n",
      "08:29:29 - Training round 64/1000\n",
      "08:29:30 - Training round 65/1000\n",
      "08:29:31 - Training round 66/1000\n",
      "08:29:32 - Training round 67/1000\n",
      "08:29:33 - Training round 68/1000\n",
      "08:29:34 - Training round 69/1000\n",
      "08:29:35 - Training round 70/1000\n",
      "08:29:36 - Training round 71/1000\n",
      "08:29:37 - Evaluating models\n",
      "08:29:38 - Model update bob: Average loss: 0.0067, Accuracy: 1116/1598 (69.84%)\n",
      "08:29:38 - Model update bob: Average loss: 0.0067\n",
      "08:29:38 - Federated model: Average loss: 0.0067, Accuracy: 1116/1598 (69.84%)\n",
      "08:29:38 - Federated model: Average loss: 0.0067\n",
      "08:29:38 - Training round 72/1000\n",
      "08:29:39 - Training round 73/1000\n",
      "08:29:40 - Training round 74/1000\n",
      "08:29:41 - Training round 75/1000\n",
      "08:29:42 - Training round 76/1000\n",
      "08:29:43 - Training round 77/1000\n",
      "08:29:44 - Training round 78/1000\n",
      "08:29:45 - Training round 79/1000\n",
      "08:29:46 - Training round 80/1000\n",
      "08:29:47 - Training round 81/1000\n",
      "08:29:48 - Evaluating models\n",
      "08:29:49 - Model update bob: Average loss: 0.0068, Accuracy: 1065/1598 (66.65%)\n",
      "08:29:49 - Model update bob: Average loss: 0.0068\n",
      "08:29:49 - Federated model: Average loss: 0.0068, Accuracy: 1065/1598 (66.65%)\n",
      "08:29:49 - Federated model: Average loss: 0.0068\n",
      "08:29:49 - Training round 82/1000\n",
      "08:29:50 - Training round 83/1000\n",
      "08:29:51 - Training round 84/1000\n",
      "08:29:52 - Training round 85/1000\n",
      "08:29:53 - Training round 86/1000\n",
      "08:29:55 - Training round 87/1000\n",
      "08:29:56 - Training round 88/1000\n",
      "08:29:57 - Training round 89/1000\n",
      "08:29:58 - Training round 90/1000\n",
      "08:29:59 - Training round 91/1000\n",
      "08:30:00 - Evaluating models\n",
      "08:30:00 - Model update bob: Average loss: 0.0064, Accuracy: 1159/1598 (72.53%)\n",
      "08:30:00 - Model update bob: Average loss: 0.0064\n",
      "08:30:01 - Federated model: Average loss: 0.0064, Accuracy: 1159/1598 (72.53%)\n",
      "08:30:01 - Federated model: Average loss: 0.0064\n",
      "08:30:01 - Training round 92/1000\n",
      "08:30:02 - Training round 93/1000\n",
      "08:30:03 - Training round 94/1000\n",
      "08:30:04 - Training round 95/1000\n",
      "08:30:05 - Training round 96/1000\n",
      "08:30:06 - Training round 97/1000\n",
      "08:30:07 - Training round 98/1000\n",
      "08:30:08 - Training round 99/1000\n",
      "08:30:09 - Training round 100/1000\n",
      "08:30:10 - Training round 101/1000\n",
      "08:30:11 - Evaluating models\n",
      "08:30:12 - Model update bob: Average loss: 0.0070, Accuracy: 1071/1598 (67.02%)\n",
      "08:30:12 - Model update bob: Average loss: 0.0070\n",
      "08:30:12 - Federated model: Average loss: 0.0070, Accuracy: 1071/1598 (67.02%)\n",
      "08:30:12 - Federated model: Average loss: 0.0070\n",
      "08:30:12 - Training round 102/1000\n",
      "08:30:13 - Training round 103/1000\n",
      "08:30:14 - Training round 104/1000\n",
      "08:30:15 - Training round 105/1000\n",
      "08:30:16 - Training round 106/1000\n",
      "08:30:17 - Training round 107/1000\n",
      "08:30:18 - Training round 108/1000\n",
      "08:30:20 - Training round 109/1000\n",
      "08:30:21 - Training round 110/1000\n",
      "08:30:22 - Training round 111/1000\n",
      "08:30:23 - Evaluating models\n",
      "08:30:23 - Model update bob: Average loss: 0.0064, Accuracy: 1160/1598 (72.59%)\n",
      "08:30:23 - Model update bob: Average loss: 0.0064\n",
      "08:30:23 - Federated model: Average loss: 0.0064, Accuracy: 1160/1598 (72.59%)\n",
      "08:30:23 - Federated model: Average loss: 0.0064\n",
      "08:30:23 - Training round 112/1000\n",
      "08:30:25 - Training round 113/1000\n",
      "08:30:26 - Training round 114/1000\n",
      "08:30:27 - Training round 115/1000\n",
      "08:30:28 - Training round 116/1000\n",
      "08:30:29 - Training round 117/1000\n",
      "08:30:30 - Training round 118/1000\n",
      "08:30:31 - Training round 119/1000\n",
      "08:30:32 - Training round 120/1000\n",
      "08:30:33 - Training round 121/1000\n",
      "08:30:34 - Evaluating models\n",
      "08:30:35 - Model update bob: Average loss: 0.0062, Accuracy: 1147/1598 (71.78%)\n",
      "08:30:35 - Model update bob: Average loss: 0.0062\n",
      "08:30:35 - Federated model: Average loss: 0.0062, Accuracy: 1147/1598 (71.78%)\n",
      "08:30:35 - Federated model: Average loss: 0.0062\n",
      "08:30:35 - Training round 122/1000\n",
      "08:30:36 - Training round 123/1000\n",
      "08:30:37 - Training round 124/1000\n",
      "08:30:38 - Training round 125/1000\n",
      "08:30:39 - Training round 126/1000\n",
      "08:30:40 - Training round 127/1000\n",
      "08:30:41 - Training round 128/1000\n",
      "08:30:43 - Training round 129/1000\n",
      "08:30:44 - Training round 130/1000\n",
      "08:30:45 - Training round 131/1000\n",
      "08:30:46 - Evaluating models\n",
      "08:30:46 - Model update bob: Average loss: 0.0061, Accuracy: 1165/1598 (72.90%)\n",
      "08:30:46 - Model update bob: Average loss: 0.0061\n",
      "08:30:46 - Federated model: Average loss: 0.0061, Accuracy: 1165/1598 (72.90%)\n",
      "08:30:46 - Federated model: Average loss: 0.0061\n",
      "08:30:46 - Training round 132/1000\n",
      "08:30:47 - Training round 133/1000\n",
      "08:30:48 - Training round 134/1000\n",
      "08:30:50 - Training round 135/1000\n",
      "08:30:51 - Training round 136/1000\n",
      "08:30:52 - Training round 137/1000\n",
      "08:30:53 - Training round 138/1000\n",
      "08:30:54 - Training round 139/1000\n",
      "08:30:55 - Training round 140/1000\n",
      "08:30:56 - Training round 141/1000\n",
      "08:30:57 - Evaluating models\n",
      "08:30:57 - Model update bob: Average loss: 0.0061, Accuracy: 1168/1598 (73.09%)\n",
      "08:30:57 - Model update bob: Average loss: 0.0061\n",
      "08:30:58 - Federated model: Average loss: 0.0061, Accuracy: 1168/1598 (73.09%)\n",
      "08:30:58 - Federated model: Average loss: 0.0061\n",
      "08:30:58 - Training round 142/1000\n",
      "08:30:59 - Training round 143/1000\n",
      "08:31:00 - Training round 144/1000\n",
      "08:31:01 - Training round 145/1000\n",
      "08:31:02 - Training round 146/1000\n",
      "08:31:03 - Training round 147/1000\n",
      "08:31:04 - Training round 148/1000\n",
      "08:31:05 - Training round 149/1000\n",
      "08:31:06 - Training round 150/1000\n",
      "08:31:07 - Training round 151/1000\n",
      "08:31:09 - Evaluating models\n",
      "08:31:09 - Model update bob: Average loss: 0.0064, Accuracy: 1157/1598 (72.40%)\n",
      "08:31:09 - Model update bob: Average loss: 0.0064\n",
      "08:31:09 - Federated model: Average loss: 0.0064, Accuracy: 1157/1598 (72.40%)\n",
      "08:31:09 - Federated model: Average loss: 0.0064\n",
      "08:31:09 - Training round 152/1000\n",
      "08:31:10 - Training round 153/1000\n",
      "08:31:11 - Training round 154/1000\n",
      "08:31:12 - Training round 155/1000\n",
      "08:31:13 - Training round 156/1000\n",
      "08:31:15 - Training round 157/1000\n",
      "08:31:16 - Training round 158/1000\n",
      "08:31:17 - Training round 159/1000\n",
      "08:31:18 - Training round 160/1000\n",
      "08:31:19 - Training round 161/1000\n",
      "08:31:20 - Evaluating models\n",
      "08:31:20 - Model update bob: Average loss: 0.0062, Accuracy: 1174/1598 (73.47%)\n",
      "08:31:20 - Model update bob: Average loss: 0.0062\n",
      "08:31:21 - Federated model: Average loss: 0.0062, Accuracy: 1174/1598 (73.47%)\n",
      "08:31:21 - Federated model: Average loss: 0.0062\n",
      "08:31:21 - Training round 162/1000\n",
      "08:31:22 - Training round 163/1000\n",
      "08:31:23 - Training round 164/1000\n",
      "08:31:24 - Training round 165/1000\n",
      "08:31:25 - Training round 166/1000\n",
      "08:31:26 - Training round 167/1000\n",
      "08:31:27 - Training round 168/1000\n",
      "08:31:28 - Training round 169/1000\n",
      "08:31:29 - Training round 170/1000\n",
      "08:31:30 - Training round 171/1000\n",
      "08:31:31 - Evaluating models\n",
      "08:31:32 - Model update bob: Average loss: 0.0061, Accuracy: 1166/1598 (72.97%)\n",
      "08:31:32 - Model update bob: Average loss: 0.0061\n",
      "08:31:32 - Federated model: Average loss: 0.0061, Accuracy: 1166/1598 (72.97%)\n",
      "08:31:32 - Federated model: Average loss: 0.0061\n",
      "08:31:32 - Training round 172/1000\n",
      "08:31:33 - Training round 173/1000\n",
      "08:31:34 - Training round 174/1000\n",
      "08:31:35 - Training round 175/1000\n",
      "08:31:36 - Training round 176/1000\n",
      "08:31:37 - Training round 177/1000\n",
      "08:31:38 - Training round 178/1000\n",
      "08:31:39 - Training round 179/1000\n",
      "08:31:40 - Training round 180/1000\n",
      "08:31:41 - Training round 181/1000\n",
      "08:31:43 - Evaluating models\n",
      "08:31:43 - Model update bob: Average loss: 0.0061, Accuracy: 1190/1598 (74.47%)\n",
      "08:31:43 - Model update bob: Average loss: 0.0061\n",
      "08:31:43 - Federated model: Average loss: 0.0061, Accuracy: 1190/1598 (74.47%)\n",
      "08:31:43 - Federated model: Average loss: 0.0061\n",
      "08:31:43 - Training round 182/1000\n",
      "08:31:44 - Training round 183/1000\n",
      "08:31:45 - Training round 184/1000\n",
      "08:31:46 - Training round 185/1000\n",
      "08:31:47 - Training round 186/1000\n",
      "08:31:48 - Training round 187/1000\n",
      "08:31:49 - Training round 188/1000\n",
      "08:31:50 - Training round 189/1000\n",
      "08:31:51 - Training round 190/1000\n",
      "08:31:53 - Training round 191/1000\n",
      "08:31:54 - Evaluating models\n",
      "08:31:54 - Model update bob: Average loss: 0.0061, Accuracy: 1183/1598 (74.03%)\n",
      "08:31:54 - Model update bob: Average loss: 0.0061\n",
      "08:31:54 - Federated model: Average loss: 0.0061, Accuracy: 1183/1598 (74.03%)\n",
      "08:31:54 - Federated model: Average loss: 0.0061\n",
      "08:31:54 - Training round 192/1000\n",
      "08:31:55 - Training round 193/1000\n",
      "08:31:57 - Training round 194/1000\n",
      "08:31:58 - Training round 195/1000\n",
      "08:31:59 - Training round 196/1000\n",
      "08:32:00 - Training round 197/1000\n",
      "08:32:01 - Training round 198/1000\n",
      "08:32:02 - Training round 199/1000\n",
      "08:32:03 - Training round 200/1000\n",
      "08:32:04 - Training round 201/1000\n",
      "08:32:05 - Evaluating models\n",
      "08:32:05 - Model update bob: Average loss: 0.0061, Accuracy: 1176/1598 (73.59%)\n",
      "08:32:05 - Model update bob: Average loss: 0.0061\n",
      "08:32:06 - Federated model: Average loss: 0.0061, Accuracy: 1176/1598 (73.59%)\n",
      "08:32:06 - Federated model: Average loss: 0.0061\n",
      "08:32:06 - Training round 202/1000\n",
      "08:32:07 - Training round 203/1000\n",
      "08:32:08 - Training round 204/1000\n",
      "08:32:09 - Training round 205/1000\n",
      "08:32:10 - Training round 206/1000\n",
      "08:32:11 - Training round 207/1000\n",
      "08:32:12 - Training round 208/1000\n",
      "08:32:13 - Training round 209/1000\n",
      "08:32:14 - Training round 210/1000\n",
      "08:32:15 - Training round 211/1000\n",
      "08:32:16 - Evaluating models\n",
      "08:32:17 - Model update bob: Average loss: 0.0061, Accuracy: 1182/1598 (73.97%)\n",
      "08:32:17 - Model update bob: Average loss: 0.0061\n",
      "08:32:17 - Federated model: Average loss: 0.0061, Accuracy: 1182/1598 (73.97%)\n",
      "08:32:17 - Federated model: Average loss: 0.0061\n",
      "08:32:17 - Training round 212/1000\n",
      "08:32:18 - Training round 213/1000\n",
      "08:32:19 - Training round 214/1000\n",
      "08:32:20 - Training round 215/1000\n",
      "08:32:21 - Training round 216/1000\n",
      "08:32:23 - Training round 217/1000\n",
      "08:32:24 - Training round 218/1000\n",
      "08:32:25 - Training round 219/1000\n",
      "08:32:26 - Training round 220/1000\n",
      "08:32:27 - Training round 221/1000\n",
      "08:32:28 - Evaluating models\n",
      "08:32:28 - Model update bob: Average loss: 0.0061, Accuracy: 1189/1598 (74.41%)\n",
      "08:32:28 - Model update bob: Average loss: 0.0061\n",
      "08:32:28 - Federated model: Average loss: 0.0061, Accuracy: 1189/1598 (74.41%)\n",
      "08:32:28 - Federated model: Average loss: 0.0061\n",
      "08:32:28 - Training round 222/1000\n",
      "08:32:30 - Training round 223/1000\n",
      "08:32:31 - Training round 224/1000\n",
      "08:32:32 - Training round 225/1000\n",
      "08:32:33 - Training round 226/1000\n",
      "08:32:34 - Training round 227/1000\n",
      "08:32:35 - Training round 228/1000\n",
      "08:32:36 - Training round 229/1000\n",
      "08:32:37 - Training round 230/1000\n",
      "08:32:38 - Training round 231/1000\n",
      "08:32:39 - Evaluating models\n",
      "08:32:39 - Model update bob: Average loss: 0.0061, Accuracy: 1179/1598 (73.78%)\n",
      "08:32:39 - Model update bob: Average loss: 0.0061\n",
      "08:32:40 - Federated model: Average loss: 0.0061, Accuracy: 1179/1598 (73.78%)\n",
      "08:32:40 - Federated model: Average loss: 0.0061\n",
      "08:32:40 - Training round 232/1000\n",
      "08:32:41 - Training round 233/1000\n",
      "08:32:42 - Training round 234/1000\n",
      "08:32:43 - Training round 235/1000\n",
      "08:32:44 - Training round 236/1000\n",
      "08:32:45 - Training round 237/1000\n",
      "08:32:46 - Training round 238/1000\n",
      "08:32:47 - Training round 239/1000\n",
      "08:32:48 - Training round 240/1000\n",
      "08:32:49 - Training round 241/1000\n",
      "08:32:50 - Evaluating models\n",
      "08:32:51 - Model update bob: Average loss: 0.0061, Accuracy: 1176/1598 (73.59%)\n",
      "08:32:51 - Model update bob: Average loss: 0.0061\n",
      "08:32:51 - Federated model: Average loss: 0.0061, Accuracy: 1176/1598 (73.59%)\n",
      "08:32:51 - Federated model: Average loss: 0.0061\n",
      "08:32:51 - Training round 242/1000\n",
      "08:32:52 - Training round 243/1000\n",
      "08:32:53 - Training round 244/1000\n",
      "08:32:54 - Training round 245/1000\n",
      "08:32:55 - Training round 246/1000\n",
      "08:32:56 - Training round 247/1000\n",
      "08:32:57 - Training round 248/1000\n",
      "08:32:58 - Training round 249/1000\n",
      "08:32:59 - Training round 250/1000\n",
      "08:33:01 - Training round 251/1000\n",
      "08:33:02 - Evaluating models\n",
      "08:33:02 - Model update bob: Average loss: 0.0061, Accuracy: 1191/1598 (74.53%)\n",
      "08:33:02 - Model update bob: Average loss: 0.0061\n",
      "08:33:02 - Federated model: Average loss: 0.0061, Accuracy: 1191/1598 (74.53%)\n",
      "08:33:02 - Federated model: Average loss: 0.0061\n",
      "08:33:02 - Training round 252/1000\n",
      "08:33:03 - Training round 253/1000\n",
      "08:33:04 - Training round 254/1000\n",
      "08:33:06 - Training round 255/1000\n",
      "08:33:07 - Training round 256/1000\n",
      "08:33:08 - Training round 257/1000\n",
      "08:33:09 - Training round 258/1000\n",
      "08:33:10 - Training round 259/1000\n",
      "08:33:11 - Training round 260/1000\n",
      "08:33:12 - Training round 261/1000\n",
      "08:33:13 - Evaluating models\n",
      "08:33:13 - Model update bob: Average loss: 0.0061, Accuracy: 1174/1598 (73.47%)\n",
      "08:33:13 - Model update bob: Average loss: 0.0061\n",
      "08:33:14 - Federated model: Average loss: 0.0061, Accuracy: 1174/1598 (73.47%)\n",
      "08:33:14 - Federated model: Average loss: 0.0061\n",
      "08:33:14 - Training round 262/1000\n",
      "08:33:15 - Training round 263/1000\n",
      "08:33:16 - Training round 264/1000\n",
      "08:33:17 - Training round 265/1000\n",
      "08:33:18 - Training round 266/1000\n",
      "08:33:19 - Training round 267/1000\n",
      "08:33:20 - Training round 268/1000\n",
      "08:33:21 - Training round 269/1000\n",
      "08:33:22 - Training round 270/1000\n",
      "08:33:23 - Training round 271/1000\n",
      "08:33:24 - Evaluating models\n",
      "08:33:25 - Model update bob: Average loss: 0.0061, Accuracy: 1193/1598 (74.66%)\n",
      "08:33:25 - Model update bob: Average loss: 0.0061\n",
      "08:33:25 - Federated model: Average loss: 0.0061, Accuracy: 1193/1598 (74.66%)\n",
      "08:33:25 - Federated model: Average loss: 0.0061\n",
      "08:33:25 - Training round 272/1000\n",
      "08:33:26 - Training round 273/1000\n",
      "08:33:27 - Training round 274/1000\n",
      "08:33:28 - Training round 275/1000\n",
      "08:33:29 - Training round 276/1000\n",
      "08:33:30 - Training round 277/1000\n",
      "08:33:32 - Training round 278/1000\n",
      "08:33:33 - Training round 279/1000\n",
      "08:33:34 - Training round 280/1000\n",
      "08:33:35 - Training round 281/1000\n",
      "08:33:36 - Evaluating models\n",
      "08:33:36 - Model update bob: Average loss: 0.0061, Accuracy: 1191/1598 (74.53%)\n",
      "08:33:36 - Model update bob: Average loss: 0.0061\n",
      "08:33:36 - Federated model: Average loss: 0.0061, Accuracy: 1191/1598 (74.53%)\n",
      "08:33:36 - Federated model: Average loss: 0.0061\n",
      "08:33:36 - Training round 282/1000\n",
      "08:33:37 - Training round 283/1000\n",
      "08:33:38 - Training round 284/1000\n",
      "08:33:39 - Training round 285/1000\n",
      "08:33:40 - Training round 286/1000\n",
      "08:33:42 - Training round 287/1000\n",
      "08:33:43 - Training round 288/1000\n",
      "08:33:44 - Training round 289/1000\n",
      "08:33:45 - Training round 290/1000\n",
      "08:33:46 - Training round 291/1000\n",
      "08:33:47 - Evaluating models\n",
      "08:33:47 - Model update bob: Average loss: 0.0061, Accuracy: 1190/1598 (74.47%)\n",
      "08:33:47 - Model update bob: Average loss: 0.0061\n",
      "08:33:47 - Federated model: Average loss: 0.0061, Accuracy: 1190/1598 (74.47%)\n",
      "08:33:47 - Federated model: Average loss: 0.0061\n",
      "08:33:47 - Training round 292/1000\n",
      "08:33:49 - Training round 293/1000\n",
      "08:33:50 - Training round 294/1000\n",
      "08:33:51 - Training round 295/1000\n",
      "08:33:52 - Training round 296/1000\n",
      "08:33:53 - Training round 297/1000\n",
      "08:33:54 - Training round 298/1000\n",
      "08:33:55 - Training round 299/1000\n",
      "08:33:56 - Training round 300/1000\n",
      "08:33:57 - Training round 301/1000\n",
      "08:33:58 - Evaluating models\n",
      "08:33:59 - Model update bob: Average loss: 0.0061, Accuracy: 1194/1598 (74.72%)\n",
      "08:33:59 - Model update bob: Average loss: 0.0061\n",
      "08:33:59 - Federated model: Average loss: 0.0061, Accuracy: 1194/1598 (74.72%)\n",
      "08:33:59 - Federated model: Average loss: 0.0061\n",
      "08:33:59 - Training round 302/1000\n",
      "08:34:00 - Training round 303/1000\n",
      "08:34:01 - Training round 304/1000\n",
      "08:34:02 - Training round 305/1000\n",
      "08:34:03 - Training round 306/1000\n",
      "08:34:04 - Training round 307/1000\n",
      "08:34:05 - Training round 308/1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = rwc.define_and_get_arguments(args=[])\n",
    "learning_rate = args.lr\n",
    "\n",
    "device = \"cuda\"  #torch.device(\"cpu\")\n",
    "traced_model = torch.jit.trace(model, torch.zeros([1, 1, 470], dtype=torch.float),check_trace=False)\n",
    "for curr_round in range(1, args.training_rounds + 1):\n",
    "    logger.info(\"Training round %s/%s\", curr_round, args.training_rounds)\n",
    "\n",
    "    results = await asyncio.gather(\n",
    "        *[\n",
    "            rwc.fit_model_on_worker(\n",
    "                worker=worker,\n",
    "                traced_model=traced_model,\n",
    "                batch_size=args.batch_size,\n",
    "                curr_round=curr_round,\n",
    "                max_nr_batches=args.federate_after_n_batches,\n",
    "                lr=learning_rate,\n",
    "            )\n",
    "            for worker in worker_instances\n",
    "        ]\n",
    "    )\n",
    "    models = {}\n",
    "    loss_values = {}\n",
    "\n",
    "    test_models = curr_round % 10 == 1 or curr_round == args.training_rounds\n",
    "    if test_models:\n",
    "        logger.info(\"Evaluating models\")\n",
    "        np.set_printoptions(formatter={\"float\": \"{: .0f}\".format})\n",
    "        for worker_id, worker_model, _ in results:\n",
    "            rwc.evaluate_model_on_worker(\n",
    "                model_identifier=\"Model update \" + worker_id,\n",
    "                worker=testing,\n",
    "                dataset_key=\"mnist_testing\",\n",
    "                model=worker_model,\n",
    "                nr_bins=5,\n",
    "                batch_size=128,\n",
    "                print_target_hist=False,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "    # Federate models (note that this will also change the model in models[0]\n",
    "    for worker_id, worker_model, worker_loss in results:\n",
    "        if worker_model is not None:\n",
    "            models[worker_id] = worker_model\n",
    "            loss_values[worker_id] = worker_loss\n",
    "\n",
    "    traced_model = utils.federated_avg(models)\n",
    "\n",
    "    if test_models:\n",
    "        rwc.evaluate_model_on_worker(\n",
    "            model_identifier=\"Federated model\",\n",
    "            worker=testing,\n",
    "            dataset_key=\"mnist_testing\",\n",
    "            model=traced_model,\n",
    "            nr_bins=5,\n",
    "            batch_size=128,\n",
    "            print_target_hist=False,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    # decay learning rate\n",
    "    learning_rate = max(0.9 * learning_rate, args.lr * 0.01)\n",
    "\n",
    "if args.save_model:\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asyncio.get_event_loop().run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 40 rounds of training we achieve an accuracy larger than 95% on the entire testing dataset. \n",
    "This is impressing, given that no worker has access to more than 4 digits!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
