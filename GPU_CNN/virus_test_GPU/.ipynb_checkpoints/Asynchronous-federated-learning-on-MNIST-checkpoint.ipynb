{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Asynchronous federated learning on MNIST\n",
    "\n",
    "This notebook will go through the steps to run a federated learning via websocket workers in an asynchronous way using [TrainConfig](https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/advanced/Federated%20Learning%20with%20TrainConfig/Introduction%20to%20TrainConfig.ipynb). We will use federated averaging to join the remotely trained models.\n",
    "\n",
    "Authors:\n",
    "- Silvia - GitHub [@midokura-silvia](https://github.com/midokura-silvia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning setup\n",
    "\n",
    "For a Federated Learning setup with TrainConfig we need different participants:\n",
    "\n",
    "* _Workers_: own datasets.\n",
    "\n",
    "* _Coordinator_: an entity that knows the workers and the dataset name that lives in each worker. \n",
    "\n",
    "* _Evaluator_: holds the testing data and tracks model performance \n",
    "\n",
    "Each worker is represented by two parts, a proxy local to the scheduler (websocket client worker) and the remote instance that holds the data and performs the computations. The remote part is called a websocket server worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: Start the websocket workers\n",
    "So first, we need to create the remote workers. For this, you need to run in a terminal (not possible from the notebook):\n",
    "\n",
    "```bash\n",
    "python start_websocket_servers.py\n",
    "```\n",
    "\n",
    "#### What's going on?\n",
    "\n",
    "The script will instantiate three workers, Alice, Bob and Charlie and prepare their local data. \n",
    "Each worker is set up to have a subset of the MNIST training dataset. \n",
    "Alice holds all images corresponding to the digits 0-3, \n",
    "Bob holds all images corresponding to the digits 4-6 and \n",
    "Charlie holds all images corresponding to the digits 7-9. \n",
    "\n",
    "| Worker      | Digits in local dataset | Number of samples |\n",
    "| ----------- | ----------------------- | ----------------- |\n",
    "| Alice       | 0-3                     | 24754             |\n",
    "| Bob         | 4-6                     | 17181             |\n",
    "| Charlie     | 7-9                     | 18065             |\n",
    "\n",
    "\n",
    "The evaluator will be called Testing and holds the entire MNIST testing dataset.\n",
    "\n",
    "| Evaluator   | Digits in local dataset | Number of samples |\n",
    "| ----------- | ----------------------- | ----------------- |\n",
    "| Testing     | 0-9                     | 10000             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following to see the code of the function that starts a worker\n",
    "# import run_websocket_server\n",
    "\n",
    "# print(inspect.getsource(run_websocket_server.start_websocket_server_worker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing let's first need to import dependencies, setup needed arguments and configure logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import sys\n",
    "import asyncio\n",
    "\n",
    "import syft as sy\n",
    "from syft.workers.websocket_client import WebsocketClientWorker\n",
    "from syft.frameworks.torch.fl import utils\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "import run_websocket_client as rwc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook torch\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "Namespace(batch_size=512, cuda=False, federate_after_n_batches=10, lr=1, save_model=False, seed=1, test_batch_size=128, training_rounds=1000, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "args = rwc.define_and_get_arguments(args=[])\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(torch.cuda.is_available())\n",
    "torch.manual_seed(args.seed)\n",
    "print(use_cuda)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"run_websocket_client\")\n",
    "\n",
    "if not len(logger.handlers):\n",
    "    FORMAT = \"%(asctime)s - %(message)s\"\n",
    "    DATE_FMT = \"%H:%M:%S\"\n",
    "    formatter = logging.Formatter(FORMAT, DATE_FMT)\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.propagate = False\n",
    "LOG_LEVEL = logging.DEBUG\n",
    "logger.setLevel(LOG_LEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's instantiate the websocket client workers, our local proxies to the remote workers.\n",
    "Note that **this step will fail, if the websocket server workers are not running**.\n",
    "\n",
    "The workers Alice, Bob and Charlie will perform the training, wheras the testing worker hosts the test data and performs the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kwargs_websocket = {\"host\": \"106.12.19.48\", \"hook\": hook, \"verbose\": args.verbose}\n",
    "#alice = WebsocketClientWorker(id=\"alice\", port=28032, **kwargs_websocket)\n",
    "\n",
    "kwargs_websocket = {\"host\": \"106.12.19.48\", \"hook\": hook, \"verbose\": args.verbose}\n",
    "testing = WebsocketClientWorker(id=\"testing\", port=28007, **kwargs_websocket)\n",
    "\n",
    "kwargs_websocket = {\"host\": \"106.12.19.48\", \"hook\": hook, \"verbose\": args.verbose}\n",
    "bob = WebsocketClientWorker(id=\"bob\", port=28048 , **kwargs_websocket)\n",
    "#charlie = WebsocketClientWorker(id=\"charlie\", port=8779, **kwargs_websocket)\n",
    "\n",
    "worker_instances = [ bob]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Let's instantiate the machine learning model. It is a small neural network with 2 convolutional and two fully connected layers. \n",
    "It uses ReLU activations and max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(inspect.getsource(rwc.Net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=470, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc4): Linear(in_features=200, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = rwc.Net().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the model serializable\n",
    "\n",
    "In order to send the model to the workers we need the model to be serializable, for this we use [`jit`](https://pytorch.org/docs/stable/jit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(model, torch.zeros([1,1,22,22], dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start the training\n",
    "\n",
    "Now we are ready to start the federated training. We will perform training over a given number of batches separately on each worker and then calculate the federated average of the resulting model.\n",
    "\n",
    "Every 10th training round we will evaluate the performance of the models returned by the workers and of the model obtained by federated averaging. \n",
    "\n",
    "The performance will be given both as the accuracy (ratio of correct predictions) and as the histograms of predicted digits. This is of interest, as each worker only owns a subset of the digits. Therefore, in the beginning each worker will only predict their numbers and only know about the other numbers via the federated averaging process.\n",
    "\n",
    "The training is done in an asynchronous manner. This means that the scheduler just tell the workers to train and does not block to wait for the result of the training before talking to the next worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the training are given in the arguments. \n",
    "Each worker will train on a given number of batches, given by the value of federate_after_n_batches.\n",
    "The training batch size and learning rate are also configured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federate_after_n_batches: 10\n",
      "Batch size: 512\n",
      "Initial learning rate: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Federate_after_n_batches: \" + str(args.federate_after_n_batches))\n",
    "print(\"Batch size: \" + str(args.batch_size))\n",
    "print(\"Initial learning rate: \" + str(args.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:21:10 - Training round 1/1000\n"
     ]
    },
    {
     "ename": "ConnectionClosedOK",
     "evalue": "code = 1000 (OK), no reason",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionClosedOK\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-21259cb29165>\u001b[0m in \u001b[0;36masync-def-wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         ]\n\u001b[1;32m     21\u001b[0m     )\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mloss_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/android_virus_GPU/GPU_virus/virus_test_GPU/run_websocket_client.py\u001b[0m in \u001b[0;36mfit_model_on_worker\u001b[0;34m(worker, traced_model, batch_size, curr_round, max_nr_batches, lr)\u001b[0m\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m     \u001b[0mtrain_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mnist\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_ptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/envs/dl/lib/python3.7/site-packages/syft/workers/websocket_client.py\u001b[0m in \u001b[0;36masync_fit\u001b[0;34m(self, dataset_key, return_ids)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mserialized_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0mwebsocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinascii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexlify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mwebsocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# returned value will be None, so don't care\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# Reopen the standard connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/envs/dl/lib/python3.7/site-packages/websockets/protocol.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    507\u001b[0m                     \u001b[0;31m# Wait until the connection is closed to raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m                     \u001b[0;31m# ConnectionClosed with the correct code and reason.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                     \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;31m# Pop a message from the queue.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/envs/dl/lib/python3.7/site-packages/websockets/protocol.py\u001b[0m in \u001b[0;36mensure_open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0;31m# case self.close_connection_task will complete even faster.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshield\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_connection_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection_closed_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;31m# Control may only reach this point in buggy third-party subclasses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionClosedOK\u001b[0m: code = 1000 (OK), no reason"
     ]
    }
   ],
   "source": [
    "\n",
    "args = rwc.define_and_get_arguments(args=[])\n",
    "learning_rate = args.lr\n",
    "\n",
    "device = \"cuda\"  #torch.device(\"cpu\")\n",
    "traced_model = torch.jit.trace(model, torch.zeros([1,1,22,22], dtype=torch.float).to(device),check_trace=False)\n",
    "for curr_round in range(1, args.training_rounds + 1):\n",
    "    logger.info(\"Training round %s/%s\", curr_round, args.training_rounds)\n",
    "\n",
    "    results = await asyncio.gather(\n",
    "        *[\n",
    "            rwc.fit_model_on_worker(\n",
    "                worker=worker,\n",
    "                traced_model=traced_model,\n",
    "                batch_size=args.batch_size,\n",
    "                curr_round=curr_round,\n",
    "                max_nr_batches=args.federate_after_n_batches,\n",
    "                lr=learning_rate,\n",
    "            )\n",
    "            for worker in worker_instances\n",
    "        ]\n",
    "    )\n",
    "    models = {}\n",
    "    loss_values = {}\n",
    "\n",
    "    test_models = curr_round % 10 == 1 or curr_round == args.training_rounds\n",
    "    if test_models:\n",
    "        logger.info(\"Evaluating models\")\n",
    "        np.set_printoptions(formatter={\"float\": \"{: .0f}\".format})\n",
    "        for worker_id, worker_model, _ in results:\n",
    "            rwc.evaluate_model_on_worker(\n",
    "                model_identifier=\"Model update \" + worker_id,\n",
    "                worker=testing,\n",
    "                dataset_key=\"mnist_testing\",\n",
    "                model=worker_model,\n",
    "                nr_bins=5,\n",
    "                batch_size=128,\n",
    "                print_target_hist=False,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "    # Federate models (note that this will also change the model in models[0]\n",
    "    for worker_id, worker_model, worker_loss in results:\n",
    "        if worker_model is not None:\n",
    "            models[worker_id] = worker_model\n",
    "            loss_values[worker_id] = worker_loss\n",
    "\n",
    "    traced_model = utils.federated_avg(models)\n",
    "\n",
    "    if test_models:\n",
    "        rwc.evaluate_model_on_worker(\n",
    "            model_identifier=\"Federated model\",\n",
    "            worker=testing,\n",
    "            dataset_key=\"mnist_testing\",\n",
    "            model=traced_model,\n",
    "            nr_bins=5,\n",
    "            batch_size=128,\n",
    "            print_target_hist=False,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    # decay learning rate\n",
    "    learning_rate = max(0.98 * learning_rate, args.lr * 0.01)\n",
    "\n",
    "if args.save_model:\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asyncio.get_event_loop().run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 40 rounds of training we achieve an accuracy larger than 95% on the entire testing dataset. \n",
    "This is impressing, given that no worker has access to more than 4 digits!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
